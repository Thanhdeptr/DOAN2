# Vision Mamba 2 vá»›i Mamba SSM

Dá»± Ã¡n nÃ y implement **Mamba SSM (State Space Model)** cho vision tasks, cá»¥ thá»ƒ lÃ  classification trÃªn MNIST dataset. ÄÃ¢y lÃ  má»™t implementation tá»« Ä‘áº§u cá»§a Mamba SSM dá»±a trÃªn nghiÃªn cá»©u tá»« paper vÃ  video.

## ğŸš€ Mamba SSM lÃ  gÃ¬?

**Mamba SSM** lÃ  má»™t kiáº¿n trÃºc má»›i thay tháº¿ cho Transformer, sá»­ dá»¥ng **State Space Models** Ä‘á»ƒ xá»­ lÃ½ sequences. Æ¯u Ä‘iá»ƒm chÃ­nh:

- **Linear complexity**: O(n) thay vÃ¬ O(nÂ²) nhÆ° Transformer
- **Long sequence handling**: CÃ³ thá»ƒ xá»­ lÃ½ sequences dÃ i hÆ¡n hiá»‡u quáº£
- **Selective scanning**: Chá»‰ focus vÃ o nhá»¯ng pháº§n quan trá»ng cá»§a input
- **Memory efficient**: Ãt memory hÆ¡n so vá»›i attention mechanisms

## ğŸ“ Cáº¥u trÃºc Project

```
vision_mamba2_mnist/
â”œâ”€â”€ mamba_ssm_simple.py      # Implementation Mamba SSM tá»« Ä‘áº§u
â”œâ”€â”€ train_mamba.py           # Script training cho Mamba SSM
â”œâ”€â”€ demo_mamba.py            # Demo vÃ  analysis
â”œâ”€â”€ mamba_ssm.py             # PhiÃªn báº£n Mamba SSM Ä‘áº§u tiÃªn (cÃ³ lá»—i)
â”œâ”€â”€ vision_simple.py         # Model LSTM cÅ© (backup)
â”œâ”€â”€ train_simple.py          # Training script cho LSTM
â”œâ”€â”€ demo.py                  # Demo cho LSTM
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ README.md               # README cÅ©
â””â”€â”€ README_MAMBA.md         # README nÃ y
```

## ğŸ”§ Installation

```bash
# KÃ­ch hoáº¡t virtual environment
myenv\Scripts\activate

# CÃ i Ä‘áº·t dependencies
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
pip install numpy matplotlib tqdm tensorboard scikit-learn seaborn
```

## ğŸ—ï¸ Kiáº¿n trÃºc Mamba SSM

### 1. SelectiveScan Module
```python
class SelectiveScan(nn.Module):
    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):
        # SSM parameters: A, B, C, D
        # Convolution layer cho local interactions
        # Projection layers
```

**CÃ¡c thÃ nh pháº§n chÃ­nh:**
- **SSM Parameters**: A, B, C, D matrices cho state space modeling
- **Convolution**: Xá»­ lÃ½ local interactions
- **Gating**: Selective information flow
- **State Update**: s_t = A * s_{t-1} + B * x_t
- **Output**: y_t = C * s_t + D * x_t

### 2. MambaBlock
```python
class MambaBlock(nn.Module):
    def forward(self, x):
        # Reshape 2D -> 1D sequence
        # Apply SelectiveScan
        # Reshape back to 2D
```

### 3. VisionMamba2
```python
class VisionMamba2(nn.Module):
    def __init__(self, num_classes=10, d_model=128, num_blocks=4):
        # Initial convolution
        # Multiple Mamba blocks
        # Classification head
```

## ğŸš€ Usage

### 1. Test Implementation
```bash
python mamba_ssm_simple.py
```

### 2. Training
```bash
python train_mamba.py
```

**Hyperparameters:**
- Batch size: 64
- Learning rate: 0.001
- Epochs: 20
- Model size: 128 dimensions
- Number of blocks: 4

### 3. Demo vÃ  Analysis
```bash
python demo_mamba.py
```

**CÃ¡c tÃ­nh nÄƒng:**
- Performance analysis
- Confusion matrix
- Prediction visualization
- Interactive demo

## ğŸ“Š Káº¿t quáº£ mong Ä‘á»£i

Vá»›i Mamba SSM implementation nÃ y, báº¡n cÃ³ thá»ƒ mong Ä‘á»£i:

- **Accuracy**: ~95-98% trÃªn MNIST test set
- **Training time**: Nhanh hÆ¡n Transformer tÆ°Æ¡ng Ä‘Æ°Æ¡ng
- **Memory usage**: Tháº¥p hÆ¡n attention-based models
- **Scalability**: CÃ³ thá»ƒ scale lÃªn sequences dÃ i hÆ¡n

## ğŸ”¬ So sÃ¡nh vá»›i cÃ¡c kiáº¿n trÃºc khÃ¡c

| Architecture | Complexity | Memory | Long Sequences | Implementation |
|--------------|------------|--------|----------------|----------------|
| Transformer  | O(nÂ²)      | High   | Limited        | Attention      |
| LSTM         | O(n)       | Medium | Good           | Recurrent      |
| **Mamba SSM**| **O(n)**   | **Low**| **Excellent**  | **State Space**|

## ğŸ› ï¸ Technical Details

### State Space Model
Mamba SSM sá»­ dá»¥ng linear state space model:

```
áº¨n state: s_t = A * s_{t-1} + B * x_t
Output:   y_t = C * s_t + D * x_t
```

### Selective Scanning
- Chá»‰ process nhá»¯ng pháº§n quan trá»ng cá»§a input
- Sá»­ dá»¥ng gating mechanism Ä‘á»ƒ control information flow
- Convolution layer cho local context

### Reshaping Strategy
```python
# 2D -> 1D sequence
x = x.permute(0, 2, 3, 1)  # (batch, height, width, channels)
x = x.reshape(batch * height, width, channels)

# Apply Mamba SSM
x = self.mamba(x)

# 1D -> 2D back
x = x.reshape(batch, height, width, channels)
x = x.permute(0, 3, 1, 2)  # (batch, channels, height, width)
```

## ğŸ¯ NghiÃªn cá»©u vÃ  References

Dá»± Ã¡n nÃ y dá»±a trÃªn:
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)
- [Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model](https://arxiv.org/abs/2401.09417)
- Video tutorial: [Mamba SSM Explained](https://www.youtube.com/watch?v=HnRBLD3_k7g)

## ğŸ”„ Migration tá»« LSTM

Náº¿u báº¡n muá»‘n chuyá»ƒn tá»« LSTM sang Mamba SSM:

1. **Backup LSTM model**: ÄÃ£ cÃ³ trong branch `LSTM-backup`
2. **Test Mamba SSM**: `python mamba_ssm_simple.py`
3. **Train Mamba SSM**: `python train_mamba.py`
4. **Compare results**: Sá»­ dá»¥ng demo scripts

## ğŸ› Troubleshooting

### Lá»—i thÆ°á»ng gáº·p:

1. **CUDA out of memory**: Giáº£m batch size
2. **Slow training**: Kiá»ƒm tra device (CPU/GPU)
3. **Import errors**: Kiá»ƒm tra virtual environment

### Performance tips:

1. **Batch size**: 32-64 cho GPU, 16-32 cho CPU
2. **Learning rate**: 0.001 vá»›i AdamW optimizer
3. **Model size**: 128 dimensions cho MNIST

## ğŸ“ˆ Next Steps

1. **Scale up**: Thá»­ nghiá»‡m vá»›i datasets lá»›n hÆ¡n (CIFAR-10, ImageNet)
2. **Architecture improvements**: ThÃªm residual connections, layer normalization
3. **Hyperparameter tuning**: Grid search cho optimal parameters
4. **Multi-modal**: Káº¿t há»£p vá»›i text hoáº·c audio

## ğŸ¤ Contributing

Äá»ƒ contribute vÃ o project:

1. Fork repository
2. Táº¡o feature branch
3. Implement changes
4. Test thoroughly
5. Submit pull request

## ğŸ“„ License

MIT License - Xem file LICENSE Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

---

**Happy coding vá»›i Mamba SSM! ğŸš€**
